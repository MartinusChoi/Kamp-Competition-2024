{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== [Data Process Start] ==================== \n",
      "\n",
      "[Process Log] Loading Raw Data...\n",
      "[Process Log] Done\n",
      "\n",
      "[Process Log] Processing Nan Value...\n",
      "[Process Log] Done\n",
      "\n",
      "[Process Log] Encoding Categorical Features...\n",
      "[Process Log] Done\n",
      "\n",
      "[Process Log] Removing Outliers (IsoForest)...\n",
      "[Outlier-Remover Log] With Outliers Shape : (89753, 23)\n",
      "[Outlier-Remover Log] Without Outliers Shape : (89034, 23)\n",
      "[Process Log] Done\n",
      "\n",
      "[Process Log] T-Testing...\n",
      "[Process Log] Done\n",
      "\n",
      "[Process Log] Data Scaling (MinMaxScaler)...\n",
      "[Process Log] Done\n",
      "\n",
      "[Process Log] Train Test Spliting...\n",
      "[Process Log] Done\n",
      "\n",
      "[Process Log] Data Resampling (adasyn)...\n",
      "[Process Log] Done\n",
      "\n",
      "======================= [Done] =======================\n"
     ]
    }
   ],
   "source": [
    "from kamp.preprocess import KampDataLoader\n",
    "\n",
    "DATA_PATH = './data/경진대회용 주조 공정최적화 데이터셋.csv'\n",
    "\n",
    "data_loader = KampDataLoader(\n",
    "    path = DATA_PATH,\n",
    "\n",
    "    # 처리 안한게 더 좋았음\n",
    "    # 처리 안한 것 : 0.944\n",
    "    # 처리 한 것 : 최대 0.922\n",
    "    do_count_trend=False,\n",
    "    drop_count=False,\n",
    "\n",
    "    get_useful_p_data=True,\n",
    "    p_threshold=0.10,\n",
    "\n",
    "    outlier_method='iso',\n",
    "    iso_outlier_rate=0.008,\n",
    "\n",
    "    do_resample=True,\n",
    "    downsampled_pass_rate=1.0,\n",
    "    upsampled_fail_rate_about_pass=0.30,\n",
    "    upsample_method='adasyn',\n",
    "\n",
    "    do_pca=False,\n",
    "    # variance_rate=0.99\n",
    ")\n",
    "\n",
    "data_loader.process()\n",
    "\n",
    "data = data_loader.load()\n",
    "\n",
    "x_train = data['train_data']\n",
    "y_train = data['train_label']\n",
    "x_test = data['test_data']\n",
    "y_test=  data['test_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((89214, 17), (89214,), (17807, 17), (17807,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kamp.preprocess import load_data, NanProcessor, CatFeatureEncoder, check_fail_rate\n",
    "from kamp.preprocess import NAN_GRID, ENCODE_GRID\n",
    "from kamp.preprocess import remove_outliers_by_isoforest, DataResampler\n",
    "\n",
    "DATA_PATH = './data/경진대회용 주조 공정최적화 데이터셋.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = load_data(DATA_PATH)\n",
    "\n",
    "data = data_configs['data']\n",
    "numeric_features = data_configs['numeric_features']\n",
    "object_features = data_configs['object_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = NanProcessor(NAN_GRID).process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    def __init__(self, do_count_trend=True):\n",
    "        self.do_count_trend=True\n",
    "    \n",
    "    def get_count_trend_feature(self, data):\n",
    "        count_trend = []\n",
    "\n",
    "        for count in data['count']:\n",
    "            if (count >= 1) and (count <= 5):\n",
    "                count_trend.append(2)\n",
    "            elif (count >= 6) and (count <= 10):\n",
    "                count_trend.append(1)\n",
    "            else:\n",
    "                count_trend.append(0)\n",
    "\n",
    "        data['count_trend'] = count_trend\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def process(self, data):\n",
    "        if self.do_count_trend:\n",
    "            data = self.get_count_trend_feature(data)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FeatureEngineer(do_count_trend=True).process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CatFeatureEncoder(ENCODE_GRID).process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 데이터 로드 (Iris 데이터셋 예시)\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# 2. 데이터 표준화\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df.drop('target', axis=1))\n",
    "\n",
    "# 3. PCA 모델 적합\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# 4. 누적 설명 분산 비율 계산\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# 5. 95% 분산을 설명하는 주성분 개수 찾기\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Number of components to explain 95% of variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAProcessor:\n",
    "    def __init__(self, variance_rate):\n",
    "        self.variance_rate = variance_rate\n",
    "\n",
    "        self.pca_computer = PCA()\n",
    "    \n",
    "    def process(self, data):\n",
    "        self.pca_computer.fit(data)\n",
    "\n",
    "        explained_variance = self.pca_computer.explained_variance_\n",
    "        cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "        n_components = np.argmax(cumulative_variance >= self.variance_rate) + 1\n",
    "\n",
    "        self.pca_computer = PCA(n_components=n_components)\n",
    "\n",
    "        pca_result = self.pca_computer.fit_transform(data)\n",
    "        pca_result = pd.DataFrame(data=pca_result, columns=[f'PC{i+1}' for i in range(n_components_95)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
